{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f69303",
   "metadata": {},
   "source": [
    "### Import OpenAI API Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd8a0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acab475",
   "metadata": {},
   "source": [
    "### LLM Markdown Response Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2010e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Model To Use\n",
    "LLM_Version = 'gpt-3.5-turbo'\n",
    "def get_completion(completion_prompt,completion_model=LLM_Version):\n",
    "    \"\"\"This is a helper function to generate a promt and returns the completion of the promt\n",
    "\n",
    "    Args:\n",
    "        promt (_type_): Promt Input\n",
    "        completion_model (str): Mention the GPT LL Version. Defaults to LLM_Version.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    print(f\"Promt Input:\\n{completion_prompt}\")\n",
    "    completion_message = [{\"role\":\"user\",\n",
    "                \"content\": completion_prompt}]\n",
    "    response = openai.ChatCompletion.create(model=completion_model,\n",
    "                                            messages=completion_message,\n",
    "                                            temperature=0 #Degree Of Randomness\n",
    "                                            )\n",
    "    return \"Prompt Resonse:\\n\"+response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc52091",
   "metadata": {},
   "source": [
    "## Test Promt Is Working Or Not Using The Below Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8980585",
   "metadata": {},
   "source": [
    "\"Making a cup of tea is easy! First, you need to get some water boiling. While that's happening, grab a cup and put a tea bag in it. Once the water is hot enough, just pour it over the tea bag. Let it sit for a bit so the tea can steep. After a few minutes, take out the tea bag. If you like, you can add some sugar or milk to taste. And that's it! You've got yourself a delicious cup of tea to enjoy.\"\n",
    "\n",
    "Provided a paragraph delimited by double quotes. \n",
    "If it contains a sequence of instructions,\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions,  then simply write \"No steps provided.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c0a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_prompt='''What is the capital of Karnataka?'''\n",
    "print(get_completion(completion_prompt=temp_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521f0a1d",
   "metadata": {},
   "source": [
    "### Need To Implement Continuous Promt In A Single Cell That Will Take Input From User And Wait For The Prompt Resonse To Comple. Then Waits For The User To Stop The Conversation To Stop By Yes Or No Choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a59dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need To Fix The Below Code\n",
    "\n",
    "# async def take_user_input():\n",
    "#     text_area = widgets.Textarea(\n",
    "#         value='',\n",
    "#         placeholder='Enter your input',\n",
    "#         rows=10,\n",
    "#         layout=widgets.Layout(width='700px')\n",
    "#     )\n",
    "\n",
    "#     submit_button = widgets.Button(description='Submit')\n",
    "\n",
    "#     user_input = None\n",
    "\n",
    "#     def handle_submit(button):\n",
    "#         nonlocal user_input\n",
    "#         user_input = text_area.value\n",
    "\n",
    "#     submit_button.on_click(handle_submit)\n",
    "\n",
    "#     display(text_area, submit_button)\n",
    "\n",
    "#     # Wait for user input\n",
    "#     while user_input is None:\n",
    "#         await asyncio.sleep(0.1)\n",
    "\n",
    "#     return user_input\n",
    "\n",
    "# async def main():\n",
    "#     while True:\n",
    "#         prompt = await take_user_input()\n",
    "#         prompt = prompt.strip()\n",
    "\n",
    "#         # Exit the loop if the user wants to stop the conversation\n",
    "#         if prompt.lower() in ['yes', 'y']:\n",
    "#             print(\"Stopping the conversation...\")\n",
    "#             break\n",
    "\n",
    "#         # Get the completion response\n",
    "#         response = await get_completion(prompt)\n",
    "\n",
    "#         # Print the response\n",
    "#         print(response)\n",
    "\n",
    "# # Run the main function\n",
    "# await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Promt_Enginnering_LLM",
   "language": "python",
   "name": "promt_enginnering_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
